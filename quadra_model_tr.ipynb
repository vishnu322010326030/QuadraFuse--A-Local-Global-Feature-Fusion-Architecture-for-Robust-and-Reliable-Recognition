{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1g27wkOgRQcS4Ns4WD-wkWgbTQWv4S64b","authorship_tag":"ABX9TyOW1vSEw2vydh1pzX54OwMO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5cfLdc1ssQA","executionInfo":{"status":"ok","timestamp":1760974682957,"user_tz":300,"elapsed":14844,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"c3899332-8fbe-4609-827c-fd9b90cdf04a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Colab: install deps\n","!pip -q install torch torchvision facenet-pytorch opencv-python tqdm\n","\n","import os, re, glob, math, random, shutil\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","from PIL import Image\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, models\n","from facenet_pytorch import MTCNN\n"]},{"cell_type":"code","source":["cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"voIRZqz8t-L7","executionInfo":{"status":"ok","timestamp":1760974682969,"user_tz":300,"elapsed":9,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"c3470802-4a6d-4eda-ebb3-a0384f5309ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKivAb5GuSo4","executionInfo":{"status":"ok","timestamp":1760974683314,"user_tz":300,"elapsed":326,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"8ca5de25-ffab-46a3-e731-32f65ea22b5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/   \u001b[01;34mFaceQuadrantNet\u001b[0m/   Test1.ipynb\n"]}]},{"cell_type":"code","source":["# ====== USER CONFIG ======\n","DATA_ROOT = \"/content/drive/MyDrive/FaceQuadrantNet/Dataset\"  # <-- change if needed\n","SAVE_DIR = \"/content/drive/MyDrive/FaceQuadrantNet\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# If you don't have class folders, but a single folder with names in filenames (e.g., alexandra_01.jpg),\n","# set this True and point DATA_ROOT to that folder.\n","INFER_FROM_FILENAME = False\n","\n","# Target classes (unique, case-insensitive)\n","TARGET_CLASSES = [\"alexandra\", \"courtney\", \"elizabeth\", \"henry\", \"zac\"]\n","TARGET_CLASSES = [c.lower() for c in list(dict.fromkeys(TARGET_CLASSES))]\n","\n","IMG_SIZE = 224             # model input (square)\n","BATCH_SIZE = 16\n","EPOCHS = 15\n","LR = 1e-4\n","VAL_SPLIT = 0.15\n","USE_ALIGNMENT = True       # MTCNN face crop+align\n","SEED = 42\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mjQUk5npuO0T","executionInfo":{"status":"ok","timestamp":1760974683346,"user_tz":300,"elapsed":30,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"9245b1a6-d287-4700-833d-a14bb5d0dd4f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7eb3c4bfdab0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# MTCNN for alignment (fast, robust)\n","mtcnn = MTCNN(image_size=IMG_SIZE, margin=20, post_process=True, device=DEVICE if DEVICE==\"cuda\" else None)\n","\n","# Basic augmentation & normalization (ImageNet stats as we use a ResNet backbone)\n","train_tfms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.02),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","])\n","\n","val_tfms = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","])\n"],"metadata":{"id":"BVVGf9MUusoW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FacesQuadrantDataset(Dataset):\n","    def __init__(self, root, classes, transform, use_alignment=True, infer_from_filename=False):\n","        self.root = Path(root)\n","        self.classes = [c.lower() for c in classes]\n","        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n","        self.transform = transform\n","        self.use_alignment = use_alignment\n","        self.infer_from_filename = infer_from_filename\n","\n","        self.samples = []\n","        if not infer_from_filename:\n","            # Expect subfolders /class/*.jpg\n","            for cls in self.classes:\n","                folder = self.root/cls\n","                if not folder.exists():\n","                    print(f\"[WARN] Missing folder for class: {cls} -> {folder}\")\n","                    continue\n","                for p in folder.rglob(\"*\"):\n","                    if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"]:\n","                        self.samples.append((str(p), self.class_to_idx[cls]))\n","        else:\n","            # Single folder; infer class from filename prefix\n","            for p in self.root.rglob(\"*\"):\n","                if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"]:\n","                    name = p.stem.lower()\n","                    # take leading alphabetic chunk as candidate label\n","                    m = re.match(r\"([a-z]+)\", name)\n","                    if m:\n","                        label = m.group(1)\n","                        if label in self.class_to_idx:\n","                            self.samples.append((str(p), self.class_to_idx[label]))\n","                        else:\n","                            # skip unknown prefix\n","                            pass\n","\n","        if len(self.samples) == 0:\n","            raise RuntimeError(\"No images found. Check DATA_ROOT and folder/filename setup.\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    @staticmethod\n","    def _read_image(path):\n","        img = cv2.imread(path)\n","        assert img is not None, f\"Failed to read image: {path}\"\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        return img\n","\n","    def _align_face(self, img):\n","        # Use MTCNN to get aligned face; fallback to raw if fails\n","        pil_img = Image.fromarray(img)\n","        try:\n","            aligned = mtcnn(pil_img)\n","            if aligned is not None:\n","                # Convert Tensor (C,H,W) normalized to 0..1 -> uint8 image for downstream transforms\n","                arr = (aligned.permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n","                return arr\n","        except Exception:\n","            pass\n","        # fallback: center resize without alignment\n","        return cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n","\n","    def __getitem__(self, idx):\n","        path, label = self.samples[idx]\n","        img = self._read_image(path)\n","        img = self._align_face(img) if self.use_alignment else cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n","\n","        img_t = self.transform(img)   # Tensor (3, H, W)\n","        return img_t, label, path\n"],"metadata":{"id":"xpB_gmFauwEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QuadrantFusionNet(nn.Module):\n","    def __init__(self, num_classes=5, backbone_name=\"resnet18\", emb_dim=512):\n","        super().__init__()\n","        # Backbone\n","        if backbone_name == \"resnet18\":\n","            net = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","            feat_dim = net.fc.in_features\n","            net.fc = nn.Identity()\n","        else:\n","            raise NotImplementedError(\"Only resnet18 implemented here.\")\n","        self.backbone = net\n","        self.feat_dim = feat_dim\n","\n","        # Project to embedding (optional)\n","        self.proj = nn.Sequential(\n","            nn.Linear(feat_dim*2, emb_dim),\n","            nn.BatchNorm1d(emb_dim),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","        # Attention over 4 locals: take concat of locals -> weights over 4\n","        self.local_att = nn.Sequential(\n","            nn.Linear(feat_dim*4, 256),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(256, 4)\n","        )\n","\n","        # Classifier head for 5 identities\n","        self.classifier = nn.Linear(emb_dim, num_classes)\n","\n","    @staticmethod\n","    def _split_quadrants(x):\n","        # x: (B,3,H,W) -> 4 quads (B,3,H/2,W/2)\n","        _, _, H, W = x.shape\n","        h2, w2 = H//2, W//2\n","        TL = x[:, :, 0:h2,   0:w2]\n","        TR = x[:, :, 0:h2,   w2:W]\n","        BL = x[:, :, h2:H,   0:w2]\n","        BR = x[:, :, h2:H,   w2:W]\n","        return TL, TR, BL, BR\n","\n","    def _embed_single(self, x):\n","        # x: (B,3,H,W) -> (B,feat_dim)\n","        return self.backbone(x)\n","\n","    def _fuse_global_local(self, Eg, locals_cat, locals_list):\n","        # locals_cat: concat[E_tl, E_tr, E_bl, E_br] -> (B, 4*feat_dim)\n","        # locals_list: list of 4 tensors [(B,feat_dim),...]\n","        # Attention weights over locals\n","        B = Eg.shape[0]\n","        weights = self.local_att(locals_cat)                    # (B,4)\n","        weights = F.softmax(weights, dim=1).unsqueeze(-1)       # (B,4,1)\n","        locals_stack = torch.stack(locals_list, dim=1)          # (B,4,feat_dim)\n","        El = torch.sum(weights * locals_stack, dim=1)           # (B,feat_dim)\n","\n","        fused = torch.cat([Eg, El], dim=1)                      # (B, 2*feat_dim)\n","        fused = self.proj(fused)                                # (B, emb_dim)\n","        return fused, weights.squeeze(-1)                       # return weights for logging\n","\n","    def forward(self, x):\n","        # Full image\n","        Eg = self._embed_single(x)                              # (B,feat_dim)\n","\n","        # Quadrants\n","        TL, TR, BL, BR = self._split_quadrants(x)\n","        Etl = self._embed_single(TL)\n","        Etr = self._embed_single(TR)\n","        Ebl = self._embed_single(BL)\n","        Ebr = self._embed_single(BR)\n","\n","        locals_list = [Etl, Etr, Ebl, Ebr]\n","        locals_cat  = torch.cat(locals_list, dim=1)             # (B,4*feat_dim)\n","\n","        fused, att_w = self._fuse_global_local(Eg, locals_cat, locals_list)\n","        logits = self.classifier(fused)\n","        return logits, fused, att_w   # att_w in order [TL,TR,BL,BR]\n","\n","    @torch.no_grad()\n","    def forward_embeddings(self, x):\n","        self.eval()\n","        logits, emb, att_w = self.forward(x)\n","        return emb, att_w\n"],"metadata":{"id":"mtYOAXBNuzVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_ds = FacesQuadrantDataset(\n","    root=DATA_ROOT,\n","    classes=TARGET_CLASSES,\n","    transform=train_tfms,            # we’ll override for val later\n","    use_alignment=USE_ALIGNMENT,\n","    infer_from_filename=INFER_FROM_FILENAME\n",")\n","\n","# Train/Val split\n","val_size = max(1, int(len(full_ds)*VAL_SPLIT))\n","train_size = len(full_ds) - val_size\n","train_ds, val_ds = random_split(full_ds, [train_size, val_size],\n","                                generator=torch.Generator().manual_seed(SEED))\n","\n","# Fix val transforms (no heavy augs)\n","val_ds.dataset.transform = val_tfms\n","\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n","val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n","\n","len(full_ds), len(train_ds), len(val_ds)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoaZxJWOu1Vu","executionInfo":{"status":"ok","timestamp":1760974685150,"user_tz":300,"elapsed":1543,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"61c90634-0b07-4c72-f519-b6101a0a5e80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[WARN] Missing folder for class: zac -> /content/drive/MyDrive/FaceQuadrantNet/Dataset/zac\n"]},{"output_type":"execute_result","data":{"text/plain":["(349, 297, 52)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["model = QuadrantFusionNet(num_classes=len(TARGET_CLASSES)).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n","\n","best_val_acc = 0.0\n","ckpt_path = os.path.join(SAVE_DIR, \"quadrant_fusion_faces.pth\")\n","\n","for epoch in range(1, EPOCHS+1):\n","    # ---- Train ----\n","    model.train()\n","    train_loss, correct, total = 0.0, 0, 0\n","    for imgs, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS} [train]\"):\n","        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad(set_to_none=True)\n","        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n","            logits, fused, att_w = model(imgs)\n","            loss = criterion(logits, labels)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        train_loss += loss.item() * imgs.size(0)\n","        preds = logits.argmax(1)\n","        correct += (preds == labels).sum().item()\n","        total += imgs.size(0)\n","    train_acc = correct / total\n","    train_loss /= total\n","\n","    # ---- Validate ----\n","    model.eval()\n","    val_loss, v_correct, v_total = 0.0, 0, 0\n","    with torch.no_grad():\n","        for imgs, labels, _ in tqdm(val_loader, desc=f\"Epoch {epoch}/{EPOCHS} [val]\"):\n","            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n","            logits, fused, att_w = model(imgs)\n","            loss = criterion(logits, labels)\n","            val_loss += loss.item() * imgs.size(0)\n","            v_correct += (logits.argmax(1) == labels).sum().item()\n","            v_total += imgs.size(0)\n","    val_acc = v_correct / v_total\n","    val_loss /= v_total\n","\n","    print(f\"Epoch {epoch:02d}: train_loss={train_loss:.4f} acc={train_acc:.3f} | val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n","\n","    # Save best\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save({\n","            \"model_state\": model.state_dict(),\n","            \"classes\": TARGET_CLASSES,\n","            \"img_size\": IMG_SIZE\n","        }, ckpt_path)\n","        print(f\"✅ Saved best checkpoint @ {ckpt_path} (val_acc={val_acc:.3f})\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p6gU8LRVvC7o","executionInfo":{"status":"ok","timestamp":1760974788124,"user_tz":300,"elapsed":102972,"user":{"displayName":"vishnu pujari","userId":"08043233301898718925"}},"outputId":"19a8ed30-b270-4aa3-e088-1235c6a37756"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 223MB/s]\n","/tmp/ipython-input-1420382410.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n","Epoch 1/15 [train]:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipython-input-1420382410.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n","Epoch 1/15 [train]: 100%|██████████| 19/19 [01:06<00:00,  3.51s/it]\n","Epoch 1/15 [val]: 100%|██████████| 4/4 [00:11<00:00,  2.88s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: train_loss=0.9102 acc=0.741 | val_loss=0.8156 acc=0.692\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=0.692)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 15.05it/s]\n","Epoch 2/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 02: train_loss=0.1582 acc=0.990 | val_loss=0.3556 acc=0.904\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=0.904)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 13.28it/s]\n","Epoch 3/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 03: train_loss=0.0549 acc=0.997 | val_loss=0.5231 acc=0.865\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.71it/s]\n","Epoch 4/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 04: train_loss=0.0459 acc=1.000 | val_loss=0.3153 acc=0.885\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.72it/s]\n","Epoch 5/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 11.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 05: train_loss=0.0375 acc=0.997 | val_loss=0.2102 acc=0.942\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=0.942)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 12.92it/s]\n","Epoch 6/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 06: train_loss=0.0261 acc=1.000 | val_loss=0.1706 acc=0.962\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=0.962)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 12.98it/s]\n","Epoch 7/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 07: train_loss=0.0134 acc=1.000 | val_loss=0.1523 acc=0.981\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=0.981)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 13.33it/s]\n","Epoch 8/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 08: train_loss=0.0213 acc=1.000 | val_loss=0.2354 acc=0.904\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.65it/s]\n","Epoch 9/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 09: train_loss=0.0136 acc=1.000 | val_loss=0.2129 acc=0.942\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 13.60it/s]\n","Epoch 10/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: train_loss=0.0160 acc=1.000 | val_loss=0.1173 acc=0.981\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.55it/s]\n","Epoch 11/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11: train_loss=0.0117 acc=1.000 | val_loss=0.1189 acc=1.000\n","✅ Saved best checkpoint @ /content/drive/MyDrive/FaceQuadrantNet/quadrant_fusion_faces.pth (val_acc=1.000)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 13.37it/s]\n","Epoch 12/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12: train_loss=0.0143 acc=1.000 | val_loss=0.1874 acc=0.962\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.26it/s]\n","Epoch 13/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 11.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: train_loss=0.0133 acc=1.000 | val_loss=0.1426 acc=0.962\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 13.71it/s]\n","Epoch 14/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 12.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: train_loss=0.0098 acc=1.000 | val_loss=0.1051 acc=1.000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/15 [train]: 100%|██████████| 19/19 [00:01<00:00, 14.81it/s]\n","Epoch 15/15 [val]: 100%|██████████| 4/4 [00:00<00:00, 13.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: train_loss=0.0071 acc=1.000 | val_loss=0.1177 acc=1.000\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["ckpt_path = os.path.join(SAVE_DIR, \"quadrant_fusion_faces.pth\")"],"metadata":{"id":"RD-efvo90qaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load best ckpt (if needed later)\n","ckpt = torch.load(ckpt_path, map_location=DEVICE)\n","model.load_state_dict(ckpt[\"model_state\"])\n","model.eval()\n","\n","@torch.no_grad()\n","def preprocess_image(img_path, use_alignment=USE_ALIGNMENT):\n","    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","    if use_alignment:\n","        pil_img = Image.fromarray(img)\n","        aligned = mtcnn(pil_img)\n","        if aligned is not None:\n","            arr = (aligned.permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n","            img = arr\n","        else:\n","            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n","    else:\n","        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n","    ten = val_tfms(img).unsqueeze(0).to(DEVICE)\n","    return ten\n","\n","@torch.no_grad()\n","def predict_class(img_path):\n","    x = preprocess_image(img_path)\n","    logits, fused, att_w = model(x)\n","    prob = F.softmax(logits, dim=1)[0].cpu().numpy()\n","    pred_idx = int(np.argmax(prob))\n","    pred_cls = TARGET_CLASSES[pred_idx]\n","    return pred_cls, prob, att_w[0].cpu().numpy()  # att_w order: [TL, TR, BL, BR]\n","\n","@torch.no_grad()\n","def face_embedding(img_path):\n","    x = preprocess_image(img_path)\n","    emb, att_w = model.forward_embeddings(x)\n","    emb = F.normalize(emb, dim=1)  # L2-normalize for cosine\n","    return emb[0].cpu().numpy(), att_w[0].cpu().numpy()\n"],"metadata":{"id":"rHO853VRvJCk"},"execution_count":null,"outputs":[]}]}